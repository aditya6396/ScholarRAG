{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458a4c66-1328-4c57-87bd-c1221b142833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the paths to your text files:  sample news1.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53656f47913e4e029429f8fde55bf4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents processed and vector store created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question about the documents (or type 'exit' to quit):  summarize the document which contains the news\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "\n",
      "Answer:\n",
      "The Pinaka long-range missiles are a key part of India's evolving artillery capabilities, designed for modern, network-centric warfare. The Indian Army has integrated these missiles into its systems to enhance firepower and readiness.\n",
      "\n",
      "A recent practice firing of the Pinaka missile system was conducted at the Pokhran Field Firing Ranges in Rajasthan. This was the first known instance of the system being fired publicly since 2003. The exact date of the recent practice is not specified, but it is mentioned that a round of firing is scheduled to take place soon.\n",
      "\n",
      "The Pinaka Multi-Barrel Rocket Launcher (MBRL) system is indigenous and developed by India, combining high-volume firepower with precision targeting capabilities. It has been praised for its ability to enhance layered firepower and future readiness in modern warfare.\n",
      "\n",
      "The Pinaka system was designed after the Indian Army's experiences during the Kargil War and subsequent conflicts with Pakistan. Its development was seen as crucial to addressing rising tensions between India and Pakistan following a deadly terrorist attack in Pahalgam, Jammu and Kashmir, on April 22, where 26 people were killed.\n",
      "\n",
      "The Pinaka system features multiple barrels that can fire missiles at different angles, providing enhanced accuracy and firepower. It is capable of firing anti-tank missiles, artillery shells, and high-explosive bombs, making it a versatile asset for the Indian Army.\n",
      "\n",
      "While specific details about the recent practice are not provided, it is mentioned that this was part of India's efforts to enhance its long-range capabilities and prepare for future conflicts. The Pinaka system has been integrated into various military platforms, including aircraft carriers and naval vessels.\n",
      "\n",
      "The next round of firing is scheduled to take place in a few weeks' time, with sources requesting anonymity due to the sensitivity of the development. This development highlights India's continued efforts to bolster its military capabilities and counter potential threats from neighboring countries.\n",
      "\n",
      "In summary:\n",
      "\n",
      "* The Pinaka long-range missiles are a key part of India's artillery capabilities.\n",
      "* Developed by India, these missiles combine high-volume firepower with precision targeting capabilities.\n",
      "* Integrated into modern warfare systems for enhanced firepower and readiness.\n",
      "* The system was designed after Indian experiences in conflicts with Pakistan following the Pahalgam attack.\n",
      "* It features multiple barrels for enhanced accuracy and firepower.\n",
      "* Capable of firing anti-tank missiles, artillery shells, and high-explosive bombs.\n",
      "* Part of India's efforts to bolster its long-range capabilities.\n",
      "* Scheduled for a few weeks' time, details remain sensitive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question about the documents (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def chunk_text(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "\n",
    "def create_vector_store(text_chunks, embedder):\n",
    "    embeddings = embedder.encode(text_chunks, show_progress_bar=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings, text_chunks\n",
    "\n",
    "\n",
    "text_files = input(\"Enter the paths to your text files: \").split(\",\")\n",
    "text_files = [f.strip() for f in text_files]\n",
    "\n",
    "if text_files:\n",
    "    print(\"Processing text files...\")\n",
    "    all_chunks = []\n",
    "    for file_path in text_files:\n",
    "        if os.path.exists(file_path):\n",
    "            text = extract_text_from_file(file_path)\n",
    "            if text:\n",
    "                chunks = chunk_text(text)\n",
    "                all_chunks.extend(chunks)\n",
    "            else:\n",
    "                print(f\"No text extracted from {file_path}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    if all_chunks:\n",
    "        vector_store, embeddings, documents = create_vector_store(all_chunks, embedder)\n",
    "        print(\"Documents processed and vector store created!\")\n",
    "    else:\n",
    "        print(\"No valid documents processed.\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"No text files provided.\")\n",
    "    exit()\n",
    "\n",
    "# Query loop\n",
    "while True:\n",
    "    query = input(\"Ask a question about the documents (or type 'exit' to quit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a valid question.\")\n",
    "        continue\n",
    "\n",
    "    print(\"Generating response...\")\n",
    "   \n",
    "    query_embedding = embedder.encode([query])[0]\n",
    "\n",
    "    # Search for relevant chunks\n",
    "    D, I = vector_store.search(np.array([query_embedding]), k=3)\n",
    "    context = [documents[i] for i in I[0]]\n",
    "    context_text = \"\\n\".join(context)\n",
    "\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert assistant. Use the following context to answer the user's question accurately and concisely.\n",
    "If the context doesn't contain enough information, say so and provide a general answer if possible.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "use the following example as guide:\n",
    "Example 1:\n",
    "Article Snippet: \"A massive wildfire in California has forced thousands of residents to evacuate their homes. \n",
    "The fire, which started on Monday, has already burned through 15,000 acres and destroyed dozens of structures. \n",
    "Firefighters are struggling to contain the blaze due to strong winds and dry conditions.\"\n",
    "Summary: A massive California wildfire, burning 15,000 acres since Monday,\n",
    "has displaced thousands and destroyed structures, with firefighters battling strong winds to contain it.\n",
    "\n",
    "###instructions:\n",
    " -generate clear response according to the query i.e maintain relevancy\n",
    " -in case user asks for summarization ,generate it in atleast 10 sentences with relevancy\n",
    " -strictly maintain the specified format for generation\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"llama3.2:1b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\":{\n",
    "                    \"num_ctx\":80000,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        result = response.json()\n",
    "    \n",
    "        if \"response\" in result:\n",
    "            print(\"\\nAnswer:\")\n",
    "            print(result[\"response\"])\n",
    "        else:\n",
    "            print(\"\\nError from Ollama:\")\n",
    "            print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response from Ollama: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1186e7a2-e33c-43a0-a9e1-5875cf5566ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news articles...\n",
      "Processing articles...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bdaec734fc43fcbd8106f2b376b8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles processed and vector store created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question about the news or request a summary (or type 'exit' to quit):  provide the summary for the fetched news\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "\n",
      "Answer:\n",
      "Okay, here’s the RAGAS-formatted evaluation of the provided QA pair:\n",
      "\n",
      "**context:** “The Eiffel Tower, constructed in 1889, is one of the most recognizable landmarks in the world.\n",
      "  Located in Paris, France, it was initially built for the 1889 World’s Fair and stands approximately 300 meters tall.”\n",
      "\n",
      "**question:** \"When was the Eiffel Tower built?\"\n",
      "\n",
      "**ground_truth:** 1889\n",
      "\n",
      "**generated_answer:** “The Eiffel Tower was built in 1889.”\n",
      "\n",
      "---\n",
      "\n",
      "Let’s proceed with another example.\n",
      "\n",
      "**context:** \"The United States' Federal Reserve System is facing increasing scrutiny regarding its role in the 2008 financial crisis.  The system's governance structure, particularly the role of the Federal Open Market Committee (FOMC), has been a subject of debate, with critics arguing it was too closely tied to the President.  Furthermore, the system's policy decisions, such as interest rate adjustments, have been linked to economic downturns.  The Fed's recent focus on inflation and its reluctance to aggressively raise rates has drawn considerable attention.\"\n",
      "\n",
      "**question:** \"What are the main criticisms of the Federal Reserve System?\"\n",
      "\n",
      "**ground_truth:** The criticisms range from concerns about its governance structure, particularly the FOMC's influence, to the system’s policy decisions’ alleged contribution to the 2008 financial crisis and a reluctance to aggressively address inflation. The Fed's recent focus on inflation and policy has also sparked debate.\n",
      "\n",
      "**generated_answer:** “Several key criticisms are levied against the Federal Reserve System. Primarily, concerns exist regarding its governance structure, specifically the role of the Federal Open Market Committee (FOMC), which has faced debate due to its close relationship with the President. Critics also argue that the system’s policy decisions, particularly regarding interest rate adjustments, have been linked to economic downturns and a reluctance to aggressively tackle inflation.  Furthermore, the Fed's recent emphasis on inflation and its cautious approach to rate hikes has generated considerable debate and scrutiny.”\n",
      "\n",
      "Do you need another example or would you like me to generate a more detailed response for a different text?\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# api_key = input(\"Enter your api key here:\")\n",
    "# if api_key:\n",
    "#     genai.configure(api_key=api_key)\n",
    "# else:\n",
    "#     print(\"Error: Please provide a valid Gemini API Key.\")\n",
    "#     exit()\n",
    "\n",
    "NEWSAPI_KEY = \"3e55c3c53330465a9ad8ae8e02030bcb\"  \n",
    "NEWSAPI_URL = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "def fetch_news_articles(query, num_articles):\n",
    "    \"\"\"Fetch news articles using NewsAPI.\"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"language\": \"en\",\n",
    "            \"sortBy\": \"relevancy\",\n",
    "            \"pageSize\": num_articles,\n",
    "            \"apiKey\": NEWSAPI_KEY\n",
    "        }\n",
    "        response = requests.get(NEWSAPI_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        articles = response.json().get(\"articles\", [])\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news from NewsAPI: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_text_from_articles(articles):\n",
    "    \"\"\"Extract relevant text from news articles.\"\"\"\n",
    "    texts = []\n",
    "    for article in articles:\n",
    "        title = article.get(\"title\", \"\")\n",
    "        description = article.get(\"description\", \"\")\n",
    "        content = article.get(\"content\", \"\")\n",
    "        # Combine title, description, and content, removing None values\n",
    "        text = \" \".join(filter(None, [title, description, content]))\n",
    "        if text.strip():\n",
    "            texts.append(text)\n",
    "    return texts\n",
    "\n",
    "def chunk_text(text):\n",
    "    \"\"\"Split text into chunks for vectorization.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "def create_vector_store(text_chunks, embedder):\n",
    "    \"\"\"Create FAISS vector store from text chunks.\"\"\"\n",
    "    embeddings = embedder.encode(text_chunks, show_progress_bar=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings, text_chunks\n",
    "\n",
    "# Fetch and process news articles\n",
    "print(\"Fetching news articles...\")\n",
    "# articles = fetch_news_articles(query=\"world news\", num_articles=10)\n",
    "articles = fetch_news_articles(\" artificial intelligence\",20)\n",
    "if not articles:\n",
    "    print(\"No articles fetched. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Processing articles...\")\n",
    "all_chunks = []\n",
    "article_texts = extract_text_from_articles(articles)\n",
    "for text in article_texts:\n",
    "    if text:\n",
    "        chunks = chunk_text(text)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "if all_chunks:\n",
    "    vector_store, embeddings, documents = create_vector_store(all_chunks, embedder)\n",
    "    print(\"Articles processed and vector store created!\")\n",
    "else:\n",
    "    print(\"No valid article texts processed.\")\n",
    "    exit()\n",
    "\n",
    "# Query loop\n",
    "while True:\n",
    "    query = input(\"Ask a question about the news or request a summary (or type 'exit' to quit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a valid question or summary request.\")\n",
    "        continue\n",
    "\n",
    "    print(\"Generating response...\")\n",
    "\n",
    "    # Encode query\n",
    "    query_embedding = embedder.encode([query])[0]\n",
    "\n",
    "    # Search for relevant chunks\n",
    "    D, I = vector_store.search(np.array([query_embedding]), k=5)  # Increased k for more context\n",
    "    context = [documents[i] for i in I[0]]\n",
    "    context_text = \"\\n\".join(context)\n",
    "\n",
    "    prompt=f\"\"\"\n",
    "You are an expert assistant. Use the following context to answer the user's question accurately and concisely.\n",
    "If the context doesn't contain enough information, say so and provide a general answer if possible.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "use the following example as guide:\n",
    "Example 1:\n",
    "Article Snippet: \"A massive wildfire in California has forced thousands of residents to evacuate their homes. \n",
    "The fire, which started on Monday, has already burned through 15,000 acres and destroyed dozens of structures. \n",
    "Firefighters are struggling to contain the blaze due to strong winds and dry conditions.\"\n",
    "Summary: A massive California wildfire, burning 15,000 acres since Monday,\n",
    "has displaced thousands and destroyed structures, with firefighters battling strong winds to contain it.\n",
    "\n",
    "###instructions:\n",
    " -generate clear response according to the query i.e maintain relevancy\n",
    " -in case user asks for summarization ,generate it in atleast 10 sentences with relevancy\n",
    " -strictly maintain the specified format for generation\n",
    "\n",
    " Evaluate the following QA pair using RAGAS format. Provide the context passage, the user's question, the expected (ground truth) answer, and the generated answer by the system.\"\n",
    "\n",
    "Here’s a complete example you can adapt:\n",
    "\n",
    "\n",
    "  \"context\": \"The Eiffel Tower, constructed in 1889, is one of the most recognizable landmarks in the world.\n",
    "   Located in Paris, France, it was initially built for the 1889 World's Fair and stands approximately 300 meters tall.\",\n",
    "  \"question\": \"When was the Eiffel Tower built?\",\n",
    "  \"ground_truth\": \"1889\",\n",
    "  \"generated_answer\": \"The Eiffel Tower was built in 1889.\"\n",
    "\n",
    " \n",
    "\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\":{\n",
    "                    \"num_ctx\":80000,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        result = response.json()\n",
    "   # model = genai.GenerativeModel(\"gemini-2.5-pro-exp-03-25\")\n",
    "    \n",
    "        if \"response\" in result:\n",
    "            print(\"\\nAnswer:\")\n",
    "            print(result[\"response\"])\n",
    "        else:\n",
    "            print(\"\\nError from Ollama:\")\n",
    "            print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response from Ollama: {str(e)}\")\n",
    "    # try:\n",
    "    #     response = model.generate_content(prompt)\n",
    "    #     print(\"\\nAnswer:\")\n",
    "    #     print(response.text)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error generating response: {str(e)}\")\n",
    "    # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2e6bbf-2526-42c1-afaa-6bc6038dfad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': {'id': 'wired', 'name': 'Wired'}, 'author': 'Will Knight', 'title': 'The AI Race Has Gotten Crowded—and China Is Closing In on the US', 'description': 'New research from Stanford suggests artificial intelligence isn’t ruled by just OpenAI and Google, as competition increases across the US, China, and France.', 'url': 'https://www.wired.com/story/stanford-study-global-artificial-intelligence-index/', 'urlToImage': 'https://media.wired.com/photos/67f06761c622bae99bb284bc/191:100/w_1280,c_limit/business_ai_race_us_china.jpg', 'publishedAt': '2025-04-07T10:00:00Z', 'content': 'Stanfords report shows Chinese AI is on the rise overall, with models from Chinese companies scoring similar to their US counterparts on the LMSYS benchmark. It notes that China publishes more AI pap… [+3227 chars]'}, {'source': {'id': 'the-verge', 'name': 'The Verge'}, 'author': 'Tina Nguyen', 'title': 'Wikipedia is using (some) generative AI now', 'description': \"Wikipedia isn't replacing their human editors with artificial intelligence yet - but they're giving them a bit of an AI boost. On Wednesday, the Wikimedia Foundation, the nonprofit that runs Wikipedia, announced that it was integrating generative AI into its …\", 'url': 'https://www.theverge.com/ai-artificial-intelligence/659222/wikipedia-generative-ai', 'urlToImage': 'https://platform.theverge.com/wp-content/uploads/sites/2/chorus/uploads/chorus_asset/file/23327570/acastro_220315_STK013_0003.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200', 'publishedAt': '2025-05-01T15:55:16Z', 'content': 'The sites human editors will have AI perform the tedious tasks that go into writing a Wikipedia article.\\r\\nThe sites human editors will have AI perform the tedious tasks that go into writing a Wikiped… [+2254 chars]'}, {'source': {'id': 'the-verge', 'name': 'The Verge'}, 'author': 'Kylie Robison', 'title': 'Most Americans don’t trust AI — or the people in charge of it', 'description': 'AI experts are feeling pretty good about the future of their field. Most Americans are not. A new report from Pew Research Center released last week shows a sharp divide in how artificial intelligence is perceived by the people building it versus the people l…', 'url': 'https://www.theverge.com/ai-artificial-intelligence/644853/pew-gallup-data-americans-dont-trust-ai', 'urlToImage': 'https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/STK_414_AI_CHATBOT_R2_CVirginia_A.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200', 'publishedAt': '2025-04-07T21:35:19Z', 'content': 'Two new studies show that the public is anxious about AI.\\r\\nTwo new studies show that the public is anxious about AI.\\r\\nAI experts are feeling pretty good about the future of their field. Most American… [+4205 chars]'}, {'source': {'id': 'the-verge', 'name': 'The Verge'}, 'author': 'Jess Weatherbed', 'title': 'Wikipedia is giving AI developers its data to fend off bot scrapers', 'description': 'Wikipedia is attempting to dissuade artificial intelligence developers from scraping the platform by releasing a dataset that’s specifically optimized for training AI models. The Wikimedia Foundation announced on Wednesday that it had partnered with Kaggle — …', 'url': 'https://www.theverge.com/news/650467/wikipedia-kaggle-partnership-ai-dataset-machine-learning', 'urlToImage': 'https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/STK_414_AI_A.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200', 'publishedAt': '2025-04-17T10:07:03Z', 'content': 'Data science platform Kaggle is hosting a Wikipedia dataset thats specifically optimized for machine learning applications.\\r\\nData science platform Kaggle is hosting a Wikipedia dataset thats specific… [+1761 chars]'}, {'source': {'id': 'the-verge', 'name': 'The Verge'}, 'author': 'Jess Weatherbed', 'title': 'OpenAI debuts new flagship AI model', 'description': 'OpenAI has introduced GPT-4.1, a successor to the GPT-4o multimodal artificial intelligence model launched by the company last year. During a livestream on Monday, OpenAI said GPT-4.1 is better than its GPT-4o reasoning model “on just about every dimension.” …', 'url': 'https://www.theverge.com/news/647896/openai-chatgpt-gpt-4-1-mini-nano-launch-availability', 'urlToImage': 'https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/STK155_OPEN_AI_2025_CVirgiia_C.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200', 'publishedAt': '2025-04-14T17:07:10Z', 'content': 'ChatGPTs latest multimodal AI model comes as OpenAI kicks GPT-5 launch down the road.\\r\\nChatGPTs latest multimodal AI model comes as OpenAI kicks GPT-5 launch down the road.\\r\\nOpenAI has introduced GPT… [+1650 chars]'}, {'source': {'id': None, 'name': 'NPR'}, 'author': 'Katia Riddle', 'title': 'The (artificial intelligence) therapist can see you now', 'description': 'Many AI products claim to deliver mental health therapy, but with little quality control. But new research suggests with the right training, AI can be effective at helping people.', 'url': 'https://www.npr.org/sections/shots-health-news/2025/04/07/nx-s1-5351312/artificial-intelligence-mental-health-therapy', 'urlToImage': 'https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2121x1193+0+110/resize/1400/quality/100/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd6%2Fd8%2F2801a4d443eebe4b8bd9d0685665%2Fgettyimages-1251572990.jpg', 'publishedAt': '2025-04-07T11:00:00Z', 'content': 'New research suggests that given the right kind of training, AI bots can deliver mental health therapy with as much efficacy as or more than human clinicians.\\r\\nThe recent study, published in the New … [+3335 chars]'}, {'source': {'id': None, 'name': 'Slashdot.org'}, 'author': 'msmash', 'title': 'Meta Blocks Apple Intelligence in iOS Apps', 'description': \"Meta has disabled Apple Intelligence features across its iOS applications, including Facebook, WhatsApp, and Threads, according to Brazilian tech blog Sorcererhat Tech. The block affects Writing Tools, which enable text creation and editing via Apple's AI, as…\", 'url': 'https://tech.slashdot.org/story/25/04/17/1539231/meta-blocks-apple-intelligence-in-ios-apps', 'urlToImage': 'https://a.fsdn.com/sd/topics/facebook_64.png', 'publishedAt': '2025-04-17T15:39:00Z', 'content': \"Apple controls iOS. If they want their OS to have access to a text field in an app on their device... they're going to get it or the app will get blocked.\\r\\nAnd honestly, if Apple and Facebook go head… [+69 chars]\"}, {'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Lakshmi Varanasi', 'title': \"Wharton has overhauled its curriculum around AI. Here's how the business school plans to train its students for the future.\", 'description': \"The University of Pennsylvania's Wharton School has unveiled a new MBA major and undergraduate concentration in artificial intelligence.\", 'url': 'https://www.businessinsider.com/wharton-business-school-upenn-ai-curriculum-2025-4', 'urlToImage': 'https://i.insider.com/67f0266fb8b41a9673fcb642?width=1200&format=jpeg', 'publishedAt': '2025-04-07T10:45:03Z', 'content': 'Wharton has launched a new \"Artificial Intelligence for Business\" concentration.David Tran Photo/Shutterstock\\r\\n<ul><li>Wharton has introduced a new concentration for undergrads and a major for MBA st… [+4990 chars]'}, {'source': {'id': 'wired', 'name': 'Wired'}, 'author': 'Kylie Robison', 'title': 'Welcome to Sam Altman’s Orb Store', 'description': 'World opened the doors to its new San Francisco storefront with eight brand-new orbs ready for eyeballs to scan.', 'url': 'https://www.wired.com/story/sam-altman-orb-store-san-francisco/', 'urlToImage': 'https://media.wired.com/photos/6813d96f9b3c9f36f7bb0565/191:100/w_1280,c_limit/Worldcoin-Orb-SF-Store-Business-6.jpg', 'publishedAt': '2025-05-02T17:52:39Z', 'content': \"At the storefront event, a man waiting outside told me hed booked an appointment for 11:30, but noted he was an hour early, and wasn't allowed to come inside yet. He was visiting from Poland, and sai… [+1632 chars]\"}, {'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Lauren Edmonds', 'title': 'James Cameron says the cost of blockbuster films needs to be cut in half, and AI is the answer', 'description': 'James Cameron appeared on Meta\\'s \"Boz to the Future\" podcast, where he discussed how artificial intelligence could help the film industry survive.', 'url': 'https://www.businessinsider.com/james-cameron-ai-blockbuster-films-costs-2025-4', 'urlToImage': 'https://i.insider.com/67f7fcf73fe8d3928362bac8?width=1200&format=jpeg', 'publishedAt': '2025-04-10T20:16:38Z', 'content': 'Avatar director James Cameron discussed AI and film on Meta\\'s \"Boz to the Future\" podcast.Mike Marsland/Mike Marsland/WireImage\\r\\n<ul><li>Director James Cameron appeared on Meta\\'s \"Boz to the Future\" … [+3232 chars]'}, {'source': {'id': 'business-insider', 'name': 'Business Insider'}, 'author': 'Adam Rogers', 'title': 'Marc Andreessen thinks AI can do every job in the world — except his', 'description': 'The VC firm Andreessen Horowitz is betting big on artificial intelligence. But its founder thinks AI will never compete with his own brilliance.', 'url': 'https://www.businessinsider.com/marc-andreessen-ai-cant-vc-tech-investing-jobs-career-2025-5', 'urlToImage': 'https://i.insider.com/681567f5a466d2b74ab4f534?width=1200&format=jpeg', 'publishedAt': '2025-05-06T08:11:02Z', 'content': 'Marc Andreessen is, arguably, the most famous venture capitalist\\r\\n on earth. Cofounder of the legendary VC firm Andreessen Horowitz, inventor of the first popular web browser, and by reputation such … [+6385 chars]'}, {'source': {'id': None, 'name': 'Gizmodo.com'}, 'author': 'Lucas Ropek', 'title': 'Lawyer for MyPillow Founder Filed AI-Generated Brief with ‘Nearly 30’ Bogus Citations', 'description': \"For Mike Lindell's legal defense, his attorney sought help from an unusual (and unreliable) source: a chatbot.\", 'url': 'https://gizmodo.com/lawyer-for-mypillow-founder-filed-ai-generated-brief-with-nearly-30-bogus-citations-2000594743', 'urlToImage': 'https://gizmodo.com/app/uploads/2021/02/adequmxvv7mpzceodppy.jpg', 'publishedAt': '2025-04-27T16:35:15Z', 'content': 'MyPillow CEO Mike Lindell’s diehard support for Donald Trump’s election lies has landed him in multiple legal entanglements, including a case in Denver, where the pillow salesman is currently being s… [+2664 chars]'}, {'source': {'id': 'the-verge', 'name': 'The Verge'}, 'author': 'Chris Welch', 'title': 'Apple doesn’t seem too worried about Trump’s tariffs', 'description': 'Apple reported its latest quarterly earnings on Wednesday under the backdrop of a court ruling that’s poised to upend the company’s App Store business and tariff uncertainty that could spur price increases for devices including the iPhone. At least on this oc…', 'url': 'https://www.theverge.com/news/659650/apple-q2-2025-earnings-tariffs-app-store', 'urlToImage': 'https://platform.theverge.com/wp-content/uploads/sites/2/2025/03/VRG_Illo_N_Barclay_1_apple.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200', 'publishedAt': '2025-05-01T21:00:22Z', 'content': 'iPad, Mac, iPhone, and services revenue was up as Apple contends with major legal challenges and AI setbacks.\\r\\niPad, Mac, iPhone, and services revenue was up as Apple contends with major legal challe… [+2600 chars]'}, {'source': {'id': 'wired', 'name': 'Wired'}, 'author': 'Steven Levy', 'title': \"Meta's Monopoly Made It a Fair-Weather Friend\", 'description': 'As the FTC trial has shown, a lack of competition allowed the company to shift its focus away from users—and toward its bottom line.', 'url': 'https://www.wired.com/story/plaintext-ftc-meta-trial-mark-zuckerberg/', 'urlToImage': 'https://media.wired.com/photos/67fea0e8371e0649dd52a9e6/191:100/w_1280,c_limit/Plaintext-FTC-Meta-Business-2194384299.jpg', 'publishedAt': '2025-04-18T14:00:00Z', 'content': 'This week, Mark Zuckerberg took the stand in an antitrust trial that could result in the breakup of Metas social networking empire. It might be years before the nearly 3 billion users of the companys… [+4740 chars]'}]\n"
     ]
    }
   ],
   "source": [
    "print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89375c11-5029-4772-b01b-d77ee7684a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The AI Race Has Gotten Crowded—and China Is Closing In on the US New research from Stanford suggests artificial intelligence isn’t ruled by just OpenAI and Google, as competition increases across the US, China, and France. Stanfords report shows Chinese AI is on the rise overall, with models from Chinese companies scoring similar to their US counterparts on the LMSYS benchmark. It notes that China publishes more AI pap… [+3227 chars]', \"Wikipedia is using (some) generative AI now Wikipedia isn't replacing their human editors with artificial intelligence yet - but they're giving them a bit of an AI boost. On Wednesday, the Wikimedia Foundation, the nonprofit that runs Wikipedia, announced that it was integrating generative AI into its … The sites human editors will have AI perform the tedious tasks that go into writing a Wikipedia article.\\r\\nThe sites human editors will have AI perform the tedious tasks that go into writing a Wikiped… [+2254 chars]\", 'Most Americans don’t trust AI — or the people in charge of it AI experts are feeling pretty good about the future of their field. Most Americans are not. A new report from Pew Research Center released last week shows a sharp divide in how artificial intelligence is perceived by the people building it versus the people l… Two new studies show that the public is anxious about AI.\\r\\nTwo new studies show that the public is anxious about AI.\\r\\nAI experts are feeling pretty good about the future of their field. Most American… [+4205 chars]', 'Wikipedia is giving AI developers its data to fend off bot scrapers Wikipedia is attempting to dissuade artificial intelligence developers from scraping the platform by releasing a dataset that’s specifically optimized for training AI models. The Wikimedia Foundation announced on Wednesday that it had partnered with Kaggle — … Data science platform Kaggle is hosting a Wikipedia dataset thats specifically optimized for machine learning applications.\\r\\nData science platform Kaggle is hosting a Wikipedia dataset thats specific… [+1761 chars]', 'OpenAI debuts new flagship AI model OpenAI has introduced GPT-4.1, a successor to the GPT-4o multimodal artificial intelligence model launched by the company last year. During a livestream on Monday, OpenAI said GPT-4.1 is better than its GPT-4o reasoning model “on just about every dimension.” … ChatGPTs latest multimodal AI model comes as OpenAI kicks GPT-5 launch down the road.\\r\\nChatGPTs latest multimodal AI model comes as OpenAI kicks GPT-5 launch down the road.\\r\\nOpenAI has introduced GPT… [+1650 chars]', 'The (artificial intelligence) therapist can see you now Many AI products claim to deliver mental health therapy, but with little quality control. But new research suggests with the right training, AI can be effective at helping people. New research suggests that given the right kind of training, AI bots can deliver mental health therapy with as much efficacy as or more than human clinicians.\\r\\nThe recent study, published in the New … [+3335 chars]', \"Meta Blocks Apple Intelligence in iOS Apps Meta has disabled Apple Intelligence features across its iOS applications, including Facebook, WhatsApp, and Threads, according to Brazilian tech blog Sorcererhat Tech. The block affects Writing Tools, which enable text creation and editing via Apple's AI, as… Apple controls iOS. If they want their OS to have access to a text field in an app on their device... they're going to get it or the app will get blocked.\\r\\nAnd honestly, if Apple and Facebook go head… [+69 chars]\", 'Wharton has overhauled its curriculum around AI. Here\\'s how the business school plans to train its students for the future. The University of Pennsylvania\\'s Wharton School has unveiled a new MBA major and undergraduate concentration in artificial intelligence. Wharton has launched a new \"Artificial Intelligence for Business\" concentration.David Tran Photo/Shutterstock\\r\\n<ul><li>Wharton has introduced a new concentration for undergrads and a major for MBA st… [+4990 chars]', \"Welcome to Sam Altman’s Orb Store World opened the doors to its new San Francisco storefront with eight brand-new orbs ready for eyeballs to scan. At the storefront event, a man waiting outside told me hed booked an appointment for 11:30, but noted he was an hour early, and wasn't allowed to come inside yet. He was visiting from Poland, and sai… [+1632 chars]\", 'James Cameron says the cost of blockbuster films needs to be cut in half, and AI is the answer James Cameron appeared on Meta\\'s \"Boz to the Future\" podcast, where he discussed how artificial intelligence could help the film industry survive. Avatar director James Cameron discussed AI and film on Meta\\'s \"Boz to the Future\" podcast.Mike Marsland/Mike Marsland/WireImage\\r\\n<ul><li>Director James Cameron appeared on Meta\\'s \"Boz to the Future\" … [+3232 chars]', 'Marc Andreessen thinks AI can do every job in the world — except his The VC firm Andreessen Horowitz is betting big on artificial intelligence. But its founder thinks AI will never compete with his own brilliance. Marc Andreessen is, arguably, the most famous venture capitalist\\r\\n on earth. Cofounder of the legendary VC firm Andreessen Horowitz, inventor of the first popular web browser, and by reputation such … [+6385 chars]', \"Lawyer for MyPillow Founder Filed AI-Generated Brief with ‘Nearly 30’ Bogus Citations For Mike Lindell's legal defense, his attorney sought help from an unusual (and unreliable) source: a chatbot. MyPillow CEO Mike Lindell’s diehard support for Donald Trump’s election lies has landed him in multiple legal entanglements, including a case in Denver, where the pillow salesman is currently being s… [+2664 chars]\", 'Apple doesn’t seem too worried about Trump’s tariffs Apple reported its latest quarterly earnings on Wednesday under the backdrop of a court ruling that’s poised to upend the company’s App Store business and tariff uncertainty that could spur price increases for devices including the iPhone. At least on this oc… iPad, Mac, iPhone, and services revenue was up as Apple contends with major legal challenges and AI setbacks.\\r\\niPad, Mac, iPhone, and services revenue was up as Apple contends with major legal challe… [+2600 chars]', \"Meta's Monopoly Made It a Fair-Weather Friend As the FTC trial has shown, a lack of competition allowed the company to shift its focus away from users—and toward its bottom line. This week, Mark Zuckerberg took the stand in an antitrust trial that could result in the breakup of Metas social networking empire. It might be years before the nearly 3 billion users of the companys… [+4740 chars]\"]\n"
     ]
    }
   ],
   "source": [
    "print(article_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219cb5a-2791-459f-9324-65aee3e76e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "api_key = input(\"Enter your api key here:\")\n",
    "if api_key:\n",
    "    genai.configure(api_key=api_key)\n",
    "else:\n",
    "    print(\"Error: Please provide a valid Gemini API Key.\")\n",
    "    exit()\n",
    "\n",
    "# # Function to extract text from PDFs\n",
    "# def extract_text_from_pdf(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             pdf_reader = PyPDF2.PdfReader(file)\n",
    "#             text = \"\"\n",
    "#             for page in pdf_reader.pages:\n",
    "#                 extracted = page.extract_text()\n",
    "#                 if extracted:\n",
    "#                     text += extracted\n",
    "#             return text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_path}: {str(e)}\")\n",
    "#         return \"\"\n",
    "NEWSAPI_KEY = \"3e55c3c53330465a9ad8ae8e02030bcb\"  \n",
    "NEWSAPI_URL = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "def fetch_news_articles(query=\"Artificial intelligence related  news\", num_articles=10):\n",
    "    \"\"\"Fetch news articles using NewsAPI.\"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"language\": \"en\",\n",
    "            \"sortBy\": \"publishedAt\",\n",
    "            \"pageSize\": num_articles,\n",
    "            \"apiKey\": NEWSAPI_KEY\n",
    "        }\n",
    "        response = requests.get(NEWSAPI_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        articles = response.json().get(\"articles\", [])\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news from NewsAPI: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_text_from_articles(articles):\n",
    "    \"\"\"Extract relevant text from news articles.\"\"\"\n",
    "    texts = []\n",
    "    for article in articles:\n",
    "        title = article.get(\"title\", \"\")\n",
    "        description = article.get(\"description\", \"\")\n",
    "        content = article.get(\"content\", \"\")\n",
    "        # Combine title, description, and content, removing None values\n",
    "        text = \" \".join(filter(None, [title, description, content]))\n",
    "        if text.strip():\n",
    "            texts.append(text)\n",
    "    return texts\n",
    "# Function to chunk text\n",
    "def chunk_text(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Function to create vector store\n",
    "def create_vector_store(text_chunks, embedder):\n",
    "    embeddings = embedder.encode(text_chunks, show_progress_bar=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings, text_chunks\n",
    "\n",
    "# Process PDF files\n",
    "pdf_files = input(\"Enter the paths to your PDF files : \").split(\",\")\n",
    "pdf_files = [f.strip() for f in pdf_files]\n",
    "\n",
    "if pdf_files:\n",
    "    print(\"Processing PDFs...\")\n",
    "    all_chunks = []\n",
    "    for file_path in pdf_files:\n",
    "        if os.path.exists(file_path):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            if text:\n",
    "                chunks = chunk_text(text)\n",
    "                all_chunks.extend(chunks)\n",
    "            else:\n",
    "                print(f\"No text extracted from {file_path}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    # Create vector store\n",
    "    if all_chunks:\n",
    "        vector_store, embeddings, documents = create_vector_store(all_chunks, embedder)\n",
    "        print(\"Documents processed and vector store created!\")\n",
    "    else:\n",
    "        print(\"No valid documents processed.\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"No PDF files provided.\")\n",
    "    exit()\n",
    "\n",
    "# Query loop\n",
    "while True:\n",
    "    query = input(\"Ask a question about the documents (or type 'exit' to quit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a valid question.\")\n",
    "        continue\n",
    "\n",
    "    print(\"Generating response...\")\n",
    "    \n",
    "    query_embedding = embedder.encode([query])[0]\n",
    "\n",
    "    # Search for relevant chunks\n",
    "    D, I = vector_store.search(np.array([query_embedding]), k=3)\n",
    "    context = [documents[i] for i in I[0]]\n",
    "    context_text = \"\\n\".join(context)\n",
    "\n",
    "    # Prepare prompt \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert assistant. Use the following context to answer the user's question accurately and concisely.\n",
    "    If the context doesn't contain enough information, say so and provide a general answer if possible.\n",
    "\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    ###instructions:\n",
    "    -generate clear response according to the query i.e maintain relevancy\n",
    "    -in case user asks for mcq questions generation,maintain variability and uniqueness among generated response\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-2.5-pro-exp-03-25\")\n",
    "\n",
    "    # Generate response\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {str(e)}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
