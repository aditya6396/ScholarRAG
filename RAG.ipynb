{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f426206-0204-445b-827c-4d2c328ced78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.167.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\intern\\anaconda3\\lib\\site-packages (from google-generativeai) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\intern\\anaconda3\\lib\\site-packages (from google-generativeai) (2.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\intern\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\intern\\anaconda3\\lib\\site-packages (from google-generativeai) (4.11.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 17.2 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading google_api_python_client-2.167.0-py2.py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 5.5/13.2 MB 27.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.5/13.2 MB 26.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 23.7 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.24.2 google-api-python-client-2.167.0 google-auth-2.39.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.4 rsa-4.9.1 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4d0ae6-4183-4f1f-8987-2c33675b330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: pyPDF2\n",
      "Successfully installed pyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b842c3-ee05-486e-930b-25c550769c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.24-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.55 (from langchain)\n",
      "  Downloading langchain_core-0.3.55-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.33-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.16-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (2.1)\n",
      "Downloading langchain-0.3.24-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 15.9 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.55-py3-none-any.whl (434 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.33-py3-none-any.whl (358 kB)\n",
      "Downloading orjson-3.10.16-cp312-cp312-win_amd64.whl (133 kB)\n",
      "Installing collected packages: orjson, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed langchain-0.3.24 langchain-core-0.3.55 langchain-text-splitters-0.3.8 langsmith-0.3.33 orjson-3.10.16\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d30929a3-83cd-446f-b803-a20c9a56ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\intern\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from sentence_transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\intern\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\intern\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\intern\\anaconda3\\lib\\site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\intern\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\intern\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\intern\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\intern\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 7.3/10.4 MB 34.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 32.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 35.1 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed huggingface-hub-0.30.2 safetensors-0.5.3 sentence_transformers-4.1.0 tokenizers-0.21.1 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aaa2a1d-f9bd-44a3-b736-8e4d167ee61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\intern\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\intern\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\intern\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 18.5 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a20eb1e-a54e-4dc8-aabb-582b31703b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\intern\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 4.2/13.7 MB 36.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.5/13.7 MB 14.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.3/13.7 MB 21.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 21.0 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a287de13-07df-4256-a7af-73bbca0df410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Intern\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8b5920-0781-4d4b-abed-e479f034dca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your api key here: AIzaSyBPfLvjGDI-lfuujvdTdxqdNbgbuBL4v7U\n",
      "Enter the paths to your PDF files :  Manual_Goods_2024.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDFs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb71d4948454495929d0ad27c36bc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents processed and vector store created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question about the documents (or type 'exit' to quit):  give a brief explanation of the document\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "Error generating response: 404 models/gemini-2.5-pro-exp-03-25 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question about the documents (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "api_key = input(\"Enter your api key here:\")\n",
    "if api_key:\n",
    "    genai.configure(api_key=api_key)\n",
    "else:\n",
    "    print(\"Error: Please provide a valid Gemini API Key.\")\n",
    "    exit()\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                extracted = page.extract_text()\n",
    "                if extracted:\n",
    "                    text += extracted\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to chunk text\n",
    "def chunk_text(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Function to create vector store\n",
    "def create_vector_store(text_chunks, embedder):\n",
    "    embeddings = embedder.encode(text_chunks, show_progress_bar=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings, text_chunks\n",
    "\n",
    "# Process PDF files\n",
    "pdf_files = input(\"Enter the paths to your PDF files : \").split(\",\")\n",
    "pdf_files = [f.strip() for f in pdf_files]\n",
    "\n",
    "if pdf_files:\n",
    "    print(\"Processing PDFs...\")\n",
    "    all_chunks = []\n",
    "    for file_path in pdf_files:\n",
    "        if os.path.exists(file_path):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            if text:\n",
    "                chunks = chunk_text(text)\n",
    "                all_chunks.extend(chunks)\n",
    "            else:\n",
    "                print(f\"No text extracted from {file_path}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    # Create vector store\n",
    "    if all_chunks:\n",
    "        vector_store, embeddings, documents = create_vector_store(all_chunks, embedder)\n",
    "        print(\"Documents processed and vector store created!\")\n",
    "    else:\n",
    "        print(\"No valid documents processed.\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"No PDF files provided.\")\n",
    "    exit()\n",
    "\n",
    "# Query loop\n",
    "while True:\n",
    "    query = input(\"Ask a question about the documents (or type 'exit' to quit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a valid question.\")\n",
    "        continue\n",
    "\n",
    "    print(\"Generating response...\")\n",
    "    \n",
    "    query_embedding = embedder.encode([query])[0]\n",
    "\n",
    "    # Search for relevant chunks\n",
    "    D, I = vector_store.search(np.array([query_embedding]), k=3)\n",
    "    context = [documents[i] for i in I[0]]\n",
    "    context_text = \"\\n\".join(context)\n",
    "\n",
    "    # Prepare prompt \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert assistant. Use the following context to answer the user's question accurately and concisely.\n",
    "    If the context doesn't contain enough information, say so and provide a general answer if possible.\n",
    "    \"Evaluate the following QA pair using RAGAS format. Provide the context passage, the user's question, the expected (ground truth) answer, and the generated answer by the system.\"\n",
    "\n",
    "Here’s a complete example you can adapt:\n",
    "\n",
    "\n",
    "  \"context\": The Eiffel Tower, constructed in 1889, is one of the most recognizable landmarks in the world. \n",
    "              Located in Paris, France, it was initially built for the 1889 World's Fair and stands approximately 300 meters tall.,\n",
    "  \"question\": When was the Eiffel Tower built?\",\n",
    "  \"ground_truth\": \"1889\",\n",
    "  \"generated_answer\": \"The Eiffel Tower was built in 1889.\"\n",
    "\n",
    "\n",
    "\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    ###instructions:\n",
    "    -generate clear response according to the query i.e maintain relevancy\n",
    "    -in case user asks for mcq questions generation,maintain variability and uniqueness among generated response\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-2.5-pro-exp-03-25\")\n",
    "\n",
    "    # Generate response\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {str(e)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873edc36-9d7e-4cfb-a66c-4ce790965dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Intern\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the paths to your PDF files:  Manual_Goods_2024.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDFs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412b2fbee57248bc97f369b8f117d402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents processed and vector store created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question about the documents (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                extracted = page.extract_text()\n",
    "                if extracted:\n",
    "                    text += extracted\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to chunk text\n",
    "def chunk_text(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Function to create vector store\n",
    "def create_vector_store(text_chunks, embedder):\n",
    "    embeddings = embedder.encode(text_chunks, show_progress_bar=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings, text_chunks\n",
    "\n",
    "# Process PDF files\n",
    "pdf_files = input(\"Enter the paths to your PDF files: \").split(\",\")\n",
    "pdf_files = [f.strip() for f in pdf_files]\n",
    "\n",
    "if pdf_files:\n",
    "    print(\"Processing PDFs...\")\n",
    "    all_chunks = []\n",
    "    for file_path in pdf_files:\n",
    "        if os.path.exists(file_path):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            if text:\n",
    "                chunks = chunk_text(text)\n",
    "                all_chunks.extend(chunks)\n",
    "            else:\n",
    "                print(f\"No text extracted from {file_path}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    \n",
    "    if all_chunks:\n",
    "        vector_store, embeddings, documents = create_vector_store(all_chunks, embedder)\n",
    "        print(\"Documents processed and vector store created!\")\n",
    "    else:\n",
    "        print(\"No valid documents processed.\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"No PDF files provided.\")\n",
    "    exit()\n",
    "\n",
    "# Query loop\n",
    "while True:\n",
    "    query = input(\"Ask a question about the documents (or type 'exit' to quit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a valid question.\")\n",
    "        continue\n",
    "\n",
    "    print(\"Generating response...\")\n",
    "   \n",
    "    query_embedding = embedder.encode([query])[0]\n",
    "\n",
    "    # Search for relevant chunks\n",
    "    D, I = vector_store.search(np.array([query_embedding]), k=3)\n",
    "    context = [documents[i] for i in I[0]]\n",
    "    context_text = \"\\n\".join(context)\n",
    "\n",
    "    # Prepare prompt for Ollama\n",
    "    prompt = f\"\"\"\n",
    "You are an expert assistant. Use the following context to answer the user's question accurately and concisely.\n",
    "If the context doesn't contain enough information, say so and provide a general answer if possible.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "###instructions:\n",
    " -generate clear response according to the query i.e maintain relevancy\n",
    " -in case user asks for mcq questions generation,maintain variability and uniqueness among generated response\n",
    " -strictly maintain the specified format for generation\n",
    "\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"llama3.2:1b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\":{\n",
    "                    \"num_ctx\":80000,\n",
    "                    #\"temperature\": 0.5, \n",
    "                    #\"top_p\": 0.7,\n",
    "                    #\"top_k\":100,\n",
    "                    #\"max_token\"=3000\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        result = response.json()\n",
    "    \n",
    "        if \"response\" in result:\n",
    "            print(\"\\nAnswer:\")\n",
    "            print(result[\"response\"])\n",
    "        else:\n",
    "            print(\"\\nError from Ollama:\")\n",
    "            print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response from Ollama: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65af961-1b17-4fdf-b1a9-de880b2fdf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.56 (from langchain-community)\n",
      "  Downloading langchain_core-0.3.56-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.24)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.33)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.56->langchain-community)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.6/2.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.3.56-py3-none-any.whl (437 kB)\n",
      "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: typing-extensions, marshmallow, httpx-sse, typing-inspection, typing-inspect, dataclasses-json, pydantic-settings, langchain-core, langchain-community\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.55\n",
      "    Uninstalling langchain-core-0.3.55:\n",
      "      Successfully uninstalled langchain-core-0.3.55\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.23 langchain-core-0.3.56 marshmallow-3.26.1 pydantic-settings-2.9.1 typing-extensions-4.13.2 typing-inspect-0.9.0 typing-inspection-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d759cf5e-36c7-4cd3-9f44-23f2c5a1f83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\intern\\anaconda3\\lib\\site-packages (0.3.14)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.53 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-openai) (0.3.56)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-openai) (1.76.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.3.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.8.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.66.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc05887c-dffb-4bfa-be37-fcdc12397677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas\n",
      "  Downloading ragas-0.2.15-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\intern\\anaconda3\\lib\\site-packages (from ragas) (1.26.4)\n",
      "Collecting datasets (from ragas)\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\intern\\anaconda3\\lib\\site-packages (from ragas) (0.9.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\intern\\anaconda3\\lib\\site-packages (from ragas) (0.3.24)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\intern\\anaconda3\\lib\\site-packages (from ragas) (0.3.56)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\intern\\anaconda3\\lib\\site-packages (from ragas) (0.3.23)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\intern\\anaconda3\\lib\\site-packages (from ragas) (0.3.14)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\intern\\anaconda3\\lib\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\intern\\anaconda3\\lib\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from ragas) (2.8.2)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from ragas) (1.76.0)\n",
      "Collecting diskcache>=5.6.3 (from ragas)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai>1->ragas) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai>1->ragas) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai>1->ragas) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai>1->ragas) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai>1->ragas) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from openai>1->ragas) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic>=2->ragas) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic>=2->ragas) (2.20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\intern\\anaconda3\\lib\\site-packages (from datasets->ragas) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from datasets->ragas) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\intern\\anaconda3\\lib\\site-packages (from datasets->ragas) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from datasets->ragas) (2.32.3)\n",
      "Collecting xxhash (from datasets->ragas)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->ragas)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\intern\\anaconda3\\lib\\site-packages (from datasets->ragas) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from datasets->ragas) (0.30.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\intern\\anaconda3\\lib\\site-packages (from datasets->ragas) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from datasets->ragas) (6.0.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain->ragas) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain->ragas) (0.3.33)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain->ragas) (2.0.34)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core->ragas) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community->ragas) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tiktoken->ragas) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp->datasets->ragas) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp->datasets->ragas) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp->datasets->ragas) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp->datasets->ragas) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp->datasets->ragas) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from aiohttp->datasets->ragas) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (0.21.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets->ragas) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\intern\\anaconda3\\lib\\site-packages (from tqdm>4->openai>1->ragas) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pandas->datasets->ragas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from pandas->datasets->ragas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\intern\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
      "Downloading ragas-0.2.15-py3-none-any.whl (190 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, multiprocess, diskcache, datasets, ragas\n",
      "Successfully installed datasets-3.5.1 diskcache-5.6.3 multiprocess-0.70.16 ragas-0.2.15 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030f7ff-d567-462e-8794-d640c348c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f58c892-91f0-424b-a8fe-3446a4736437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6979206f-ce6f-48ea-8976-fb9bfd8bb16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    " \n",
    "# Load text data from a file using TextLoader\n",
    "loader = TextLoader(\"ragasdataset.txt\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc597f97-3c6b-440d-8f80-d7c99becbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PubMedLoader\n",
    "\n",
    "loader = PubMedLoader(\"liver\", load_max_docs=10)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c6415e-39e3-4eba-aa40-758d10fb6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76758920-5150-4087-936d-9a13bfe172c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'producer': 'pdfTeX-1.40.17',\n",
      " 'creator': 'LaTeX with hyperref package',\n",
      " 'creationdate': '2020-09-28T00:23:19+00:00',\n",
      " 'author': '',\n",
      " 'keywords': '',\n",
      " 'moddate': '2020-09-28T00:23:19+00:00',\n",
      " 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live '\n",
      "                    '2016) kpathsea version 6.2.2',\n",
      " 'subject': '',\n",
      " 'title': '',\n",
      " 'trapped': '/False',\n",
      " 'source': 'Tanogan.pdf',\n",
      " 'total_pages': 9}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\n",
    "    \"Tanogan.pdf\",\n",
    "    mode=\"single\",\n",
    ")\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "pprint.pp(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458a7e07-51c6-48e1-b122-1d72175293d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ragas.testset.generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtestset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestsetGenerator\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtestset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevolutions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple, reasoning, multi_context\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI, OpenAIEmbeddings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ragas.testset.generator'"
     ]
    }
   ],
   "source": [
    "import ragas\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import pprint\n",
    "\n",
    "# documents = load your documents\n",
    "loader = PyPDFLoader(\n",
    "    \"Tanogan.pdf\",\n",
    "    mode=\"single\",\n",
    ")\n",
    "documents = loader.load()\n",
    "print(len(docs))\n",
    "pprint.pp(docs[0].metadata)\n",
    "# generator with openai models\n",
    "generator_llm=genai.GenerativeModel(\"gemini-2.5-pro-exp-03-25\")\n",
    "#generator_llm = ChatOpenAI(model=\"g\")\n",
    "#critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "critic_llm=genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}\n",
    "\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(documents, 10, distributions) \n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f2a6f-e0df-46e5-b147-ce41af90ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = testset.to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82e096-9586-4d37-8565-c1088acc0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# data = []\n",
    "\n",
    "# with open('ragasdataset.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         line = line.strip()\n",
    "#         if not line:\n",
    "#             continue  # skip empty lines\n",
    "#         try:\n",
    "#             item = json.loads(line)\n",
    "#             data.append(item)\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             print(f\"Skipping invalid line: {line}\")\n",
    "#             continue\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e9e87-9efe-4204-a10a-642fa8737ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ragas\n",
      "Version: 0.2.15\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: \n",
      "Location: C:\\Users\\Intern\\anaconda3\\Lib\\site-packages\n",
      "Requires: appdirs, datasets, diskcache, langchain, langchain-community, langchain-core, langchain_openai, nest-asyncio, numpy, openai, pydantic, tiktoken\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip show ragas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd1e057c-3cd6-4e82-95ab-60937b7c6ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['retrieved_contexts', 'user_input', 'ground_truth', 'response'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "json_path = \"ragas.json\"\n",
    "dataset = load_dataset(\"json\", data_files=json_path)[\"train\"]\n",
    "dataset\n",
    "# import pandas as pd\n",
    "# dataset=pd.read_json(\"ragas.json\")\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9a842d-8945-4714-b29f-297c93f5566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load your current JSON (list format)\n",
    "with open(\"ragas.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Fix 'retrieved_contexts' to be a list (e.g., split into chunks or just wrap in list)\n",
    "for item in data:\n",
    "    if isinstance(item.get(\"retrieved_contexts\"), str):\n",
    "        item[\"retrieved_contexts\"] = [item[\"retrieved_contexts\"]]\n",
    "\n",
    "# Save as JSONL\n",
    "with open(\"ragas_fixed.jsonl\", \"w\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916435a-051c-4ab0-82a3-49d4fa741e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "889f0481-bac1-4c74-9d53-0bbafa05fcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response'], len=10)\n"
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "#data_list = dataset.to_dict()\n",
    "# data_list = [\n",
    "#     {\n",
    "#         \"user_input\": item[\"user_input\"],\n",
    "#         \"retrieved_contexts\": item[\"retrieved_contexts\"],\n",
    "#         \"response\": item[\"response\"],\n",
    "#         \"reference\": item.get(\"ground_truth\")\n",
    "#     }\n",
    "#     for item in data_list\n",
    "# ]\n",
    "\n",
    "ragas_dataset = EvaluationDataset.from_jsonl(\"ragas_fixed.jsonl\")\n",
    "print(ragas_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b2493-69e2-4c61-b80f-9afa27f6b767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d2e8e0f5ad451cab9d1fc98044642a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[15]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[8]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[4]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[10]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[14]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[5]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[6]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[7]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[11]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[13]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[12]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[9]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[16]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 17\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# data_list = dataset.to_dict()\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# ragas_dataset = EvaluationDataset.from_list(data_list)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#ragas_dataset = EvaluationDataset.from_hf_dataset(dataset)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [answer_relevancy,faithfulness,]\n\u001b[1;32m---> 17\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate(ragas_dataset,metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ragas\\_analytics.py:227\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m    226\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m--> 227\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    228\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ragas\\evaluation.py:294\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[0m\n\u001b[0;32m    291\u001b[0m scores: t\u001b[38;5;241m.\u001b[39mList[t\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, t\u001b[38;5;241m.\u001b[39mAny]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m     results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ragas\\executor.py:213\u001b[0m, in \u001b[0;36mExecutor.results\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m             nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m    211\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nest_asyncio_applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m results \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_jobs())\n\u001b[0;32m    214\u001b[0m sorted_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m     heappop(scheduled)\n\u001b[0;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\asyncio\\windows_events.py:445\u001b[0m, in \u001b[0;36mIocpProactor.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results:\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n\u001b[0;32m    446\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\asyncio\\windows_events.py:774\u001b[0m, in \u001b[0;36mIocpProactor._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout too big\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     status \u001b[38;5;241m=\u001b[39m _overlapped\u001b[38;5;241m.\u001b[39mGetQueuedCompletionStatus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iocp, ms)\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    776\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[18]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[17]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[19]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ragas\n",
    "from ragas import EvaluationDataset\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from ragas import evaluate\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # Replace with your key\n",
    "# data_list = dataset.to_dict()\n",
    "# ragas_dataset = EvaluationDataset.from_list(data_list)\n",
    "\n",
    "#ragas_dataset = EvaluationDataset.from_hf_dataset(dataset)\n",
    "\n",
    "metrics = [answer_relevancy,faithfulness,]\n",
    "\n",
    "\n",
    "results = evaluate(ragas_dataset,metrics=metrics)\n",
    "\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74d11cf4-c3a6-4c52-a1c8-73c35266017d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618b2f8-31a6-440c-856c-25575c1087a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09f293c8-8f36-41e0-910d-3fc28d8bed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from ragas import EvaluationDataset\n",
    "# dataset = pd.(\"ragasdataset.txt\")\n",
    "# data_list = dataset.to_dict(orient=\"records\")\n",
    "# evaluation_dataset = EvaluationDataset.from_list(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b60d16-8820-4b06-bef7-23ad076dd23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Which of the below does not constitute a cybercrime?',\n",
       "  'correctAnswer': 'Ethical Hacking',\n",
       "  'ic1': 'Unauthorized Access',\n",
       "  'ic2': 'DoS Attacks',\n",
       "  'ic3': 'Phishing',\n",
       "  'ic4': 'Encryption Development',\n",
       "  'questionExplanation': 'Conducting authorized security testing to identify and fix vulnerabilities.',\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'What do you call a malware that encrypts your files and requests a ransom to unlock them?',\n",
       "  'correctAnswer': 'Ransomware',\n",
       "  'ic1': 'Worm',\n",
       "  'ic2': 'Spyware',\n",
       "  'ic3': 'Trojan',\n",
       "  'ic4': 'Virus',\n",
       "  'questionExplanation': 'Ransomware encrypts data and demands payment for decryption.',\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'What does SSL stand for in the context of cybersecurity?',\n",
       "  'correctAnswer': 'Secure Sockets Layer',\n",
       "  'ic1': 'Secure Socks',\n",
       "  'ic2': 'Authentication Protocol',\n",
       "  'ic3': 'Data Encryption',\n",
       "  'ic4': 'Secure Layer',\n",
       "  'questionExplanation': 'SSL was the original cryptographic protocol developed by Netscape in the mid-1990s to secure communication over the internet',\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'What type of attack floods a server with junk traffic to make it unavailable?',\n",
       "  'correctAnswer': 'DoS',\n",
       "  'ic1': 'Juice Jacking',\n",
       "  'ic2': 'Phishing',\n",
       "  'ic3': 'Encryption',\n",
       "  'ic4': 'All of above',\n",
       "  'questionExplanation': 'The type of attack that floods a server with junk traffic to make it unavailable is called a Denial of Service (DoS) attack.',\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'What is the primary function of an Intrusion Detection System (IDS)?',\n",
       "  'correctAnswer': 'Threat detection',\n",
       "  'ic1': 'Firewall configuration',\n",
       "  'ic2': 'Network monitoring',\n",
       "  'ic3': 'Secure authentication',\n",
       "  'ic4': 'Data encryption',\n",
       "  'questionExplanation': nan,\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'What is the purpose of a honey pot in cybersecurity?',\n",
       "  'correctAnswer': 'To detect unauthorized access and log intrusion attempts.',\n",
       "  'ic1': 'To filter spam emails and malicious attachments.',\n",
       "  'ic2': 'To encrypt sensitive data during transmission.',\n",
       "  'ic3': 'To monitor network traffic for performance optimization.',\n",
       "  'ic4': 'All of above',\n",
       "  'questionExplanation': nan,\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'What is a common method used for authenticating users in a computer system?',\n",
       "  'correctAnswer': 'Biometrics',\n",
       "  'ic1': 'Cookies',\n",
       "  'ic2': 'Firewalls',\n",
       "  'ic3': 'Malware',\n",
       "  'ic4': 'GPS',\n",
       "  'questionExplanation': 'Biometrics involves the use of unique physical or behavioral characteristics, such as fingerprints or facial recognition, to authenticate and identify individuals.',\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'What is a common method for phishing attacks?',\n",
       "  'correctAnswer': 'Social Engineering ',\n",
       "  'ic1': 'Cross-site scripting (XSS)',\n",
       "  'ic2': 'Denial-of-Service (DoS) attacks',\n",
       "  'ic3': 'SQL injection',\n",
       "  'ic4': 'Distributed Denial-of-Service (DDoS) attacks',\n",
       "  'questionExplanation': nan,\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'What encrypts data during transmission?',\n",
       "  'correctAnswer': 'SSL',\n",
       "  'ic1': 'DNS',\n",
       "  'ic2': 'SNMP',\n",
       "  'ic3': 'DHCP',\n",
       "  'ic4': 'SMTP',\n",
       "  'questionExplanation': 'SSL (Secure Sockets Layer) ensures secure data transmission.',\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'Which factor authenticates using a physical characteristic?',\n",
       "  'correctAnswer': 'Biometric',\n",
       "  'ic1': 'Token',\n",
       "  'ic2': 'PIN',\n",
       "  'ic3': 'Password',\n",
       "  'ic4': 'CAPTCHA',\n",
       "  'questionExplanation': 'Biometrics use physical traits for authentication.',\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'Which scans for vulnerabilities in a network?',\n",
       "  'correctAnswer': 'Vulnerability Scanner',\n",
       "  'ic1': 'Intrusion Detection System (IDS)',\n",
       "  'ic2': 'Virtual Private Network (VPN)',\n",
       "  'ic3': 'Domain Name System (DNS)',\n",
       "  'ic4': 'Sandbox',\n",
       "  'questionExplanation': 'Penetration tests identify and exploit vulnerabilities.',\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'What is a common social engineering technique?',\n",
       "  'correctAnswer': 'Phishing',\n",
       "  'ic1': 'Ping of Death',\n",
       "  'ic2': 'Brute Force',\n",
       "  'ic3': 'Cross-Site Scripting',\n",
       "  'ic4': 'Man-in-the-Middle',\n",
       "  'questionExplanation': 'Spear Phishing targets specific individuals with deceptive emails.',\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'In the CIA triad of information security, what does \"I\" stand for?',\n",
       "  'correctAnswer': 'Integrity',\n",
       "  'ic1': 'Identification',\n",
       "  'ic2': 'Infiltration',\n",
       "  'ic3': 'Incognito',\n",
       "  'ic4': 'Inference',\n",
       "  'questionExplanation': 'In the CIA triad, \"Integrity\" refers to maintaining the accuracy and consistency of data, ensuring it is not tampered with or altered by unauthorized parties.',\n",
       "  'Unnamed: 7': nan},\n",
       " {'question': 'In order to ensure the security of the data/ information, we need to ____________ the data:',\n",
       "  'correctAnswer': 'Encrypt',\n",
       "  'ic1': 'Compress',\n",
       "  'ic2': 'Decrypt',\n",
       "  'ic3': 'Delete',\n",
       "  'ic4': 'None of the above',\n",
       "  'questionExplanation': nan,\n",
       "  'Unnamed: 7': nan}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[1:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef1524-e241-4f72-9e2b-68ea53b2e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "import google.generativeai as genai\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness,answer_relevancy\n",
    "llm=genai.GenerativeModel(\"gemini-2.5-pro-exp-03-25\")\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "\n",
    "result = evaluate(dataset=ragas_dataset,metrics=[answer_relevancy,Faithfulness()],llm=evaluator_llm)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50308aa-6349-4168-8d19-131e67c65c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
